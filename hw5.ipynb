{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 5: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    \n",
    "## Instructions  \n",
    "rubric={points}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Before you start  \n",
    "\n",
    "- Please **read carefully\n",
    "[Use of Generative AI policy](https://ubc-cs.github.io/cpsc330-2025W1/syllabus.html#use-of-generative-ai-in-the-course)** before starting the homework assignment. \n",
    "  \n",
    "- Review the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2025W1/docs/homework_instructions.html) for detailed guidance on completing and submitting assignments. \n",
    "\n",
    "### Group work instructions\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2.\n",
    "  \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "### Before submitting  \n",
    "\n",
    "- **Run all cells** (▶▶ button) to ensure the notebook executes cleanly from top to bottom.\n",
    "\n",
    "  - Execution counts must start at **1** and be sequential.\n",
    "    \n",
    "  - Notebooks with missing outputs or errors may lose marks.\n",
    "\n",
    "- **Do not upload or push data files** used in this lab to GitHub or Gradescope. (A `.gitignore` is provided to prevent this.)  \n",
    "\n",
    "\n",
    "### Submitting on Gradescope  \n",
    "\n",
    "- Upload **only** your `.ipynb` file (with outputs shown) and any required output files. Do **not** submit extra files.\n",
    "  \n",
    "- If needed, refer to the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- If your notebook is too large to render, also upload a **Web PDF** or **HTML** version.  \n",
    "  - You can create one using **File $\\rightarrow$ Save and Export Notebook As**.  \n",
    "  - If you get an error when creating a PDF, try running the following commands in your lab directory:  \n",
    "\n",
    "    ```bash\n",
    "    conda install -c conda-forge nbconvert-playwright\n",
    "    jupyter nbconvert --to webpdf lab1.ipynb\n",
    "    ```  \n",
    "\n",
    "  - Ensure all outputs are visible in your PDF or HTML file; TAs cannot grade your work if outputs are missing.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Unlike previous assignments, this one is open-ended and project-style. Treat it as an opportunity to explore, experiment, and learn._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Imports\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Tools and visuals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Optimization\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, auc, precision_recall_curve, confusion_matrix\n",
    "from scipy.stats import randint, loguniform\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from scipy.stats import randint, loguniform\n",
    "\n",
    "# Feature interpretation and selection\n",
    "%pip install shap\n",
    "import shap\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this homework you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (10-14 hours???) is a good guideline for this project . Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem\n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "In this mini project, you have the option to choose on which dataset you will be working on. The tasks you will need to carry on will be similar, independently of your choice.\n",
    "\n",
    "### Option 1\n",
    "You can choose to work on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "\n",
    "### Option 2\n",
    "You can choose to work on a regression problem using a [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) of New York City Airbnb listings from 2019. As usual, you'll need to start by downloading the dataset, then you will try to predict `reviews_per_month`, as a proxy for the popularity of the listing. Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here.\n",
    "\n",
    "> Note there is an updated version of this dataset with more features available [here](http://insideairbnb.com/). The features were are using in `listings.csv.gz` for the New York city datasets. You will also see some other files like `reviews.csv.gz`. For your own interest you may want to explore the expanded dataset and try your analysis there. However, please submit your results on the dataset obtained from Kaggle.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. Review the available datasets and choose the one you find most interesting. It may help to read through the dataset documentation on Kaggle before deciding.\n",
    "2. Once you've selected a dataset, take time to understand the problem it represents and the meaning of each feature. Use the Kaggle documentation to guide you.\n",
    "3. Download the dataset and load it into a pandas DataFrame.\n",
    "4. Write a few sentences summarizing your initial thoughts about the problem and the dataset.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "I chose Option 1 — predicting whether a credit card client will default. At first glance, this problem seems straightforward and well-suited for the classification models we’ve been studying, such as logistic regression and decision trees. However, after reviewing the Kaggle documentation, it becomes clear that the dataset contains several ambiguous entries that require careful preprocessing. This makes it an excellent opportunity to apply feature exploration, preprocessing, and engineering techniques learned in class. Furthermore, the real-world relevance of this problem emphasizes the importance of feature selection and interpretability, providing an opportunity to apply the feature importance and selection strategies explored in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "default_df = pd.read_csv(\"data/default.csv\")\n",
    "print(default_df.shape)\n",
    "default_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2. Data splitting\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (70%) and test (30%) portions with `random_state=123`.\n",
    "\n",
    "> If your computer cannot handle training on 70% training data, make the test split bigger.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(default_df, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. EDA\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform exploratory data analysis (EDA)**: Conduct an initial exploration of the training set to better understand its characteristics.\n",
    "\n",
    "2. **Summarize and visualize the data**: Include at least **two summary statistics** and **two visualizations** that you find informative. For each, write **one sentence** explaining what insight it provides.\n",
    "\n",
    "3. **Record your observations**: Summarize your **initial observations** about the dataset based on your EDA.\n",
    "\n",
    "4. **Select evaluation metrics**: Choose one or more **appropriate metrics** for assessing model performance and briefly justify your choice.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['EDUCATION'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MARRIAGE'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDUCATION and MARRIAGE columns contain values that are inconsistent with the Kaggle documentation. According to the dataset description, MARRIAGE should include only the categories 1, 2, and 3, and EDUCATION should range from 1 to 6; however, both features also include an undocumented category labeled 0. In addition, EDUCATION contains two “unknown” categories, 5 and 6.\n",
    "\n",
    "Several approaches could be taken to address the issues with EDUCATION: (1) dropping categories 0, 5, and 6 entirely; (2) dropping 0 and merging 5 and 6 into a single “unknown” category; or (3) merging 5 and 6 into 4 (“other”). Dropping all three is undesirable, as categories 5 and 6 together contain 245 examples, while keeping 0 is not justified, since it represents only 10 cases and is undocumented. Merging 0 with 5 and 6 is also inappropriate, as there is no evidence that 0 carries the same semantic meaning as the “unknown” categories.\n",
    "\n",
    "Merging 5 and 6 into 4 (“other”) could obscure potentially meaningful distinctions between “other” and truly “unknown” education levels. Therefore, the most reasonable approach is to drop category 0 and merge 5 and 6 into a single “unknown” category (5) although we risk losing subtle differences between the 5 and 6 categories, if any exist. Since the dataset documentation provides no further clarification, I will proceed by merging them into a single category (5) and assume that they do not contain any useful differences.\n",
    "\n",
    "For MARRIAGE, category 0 will be dropped entirely, as it contains only 34 examples and cannot be meaningfully merged with category 4 (“other”), which likely represents a distinct and more clearly defined group (e.g. divorced individuals). In contrast, category 0 appears to represent an “unknown” marital status that could include people from any marriage category, making it safer to remove to prevent noise and potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, PAY columns are reported to contain values in the range {-1, 1–9}; however, the dataset also includes the additional categories -2 and 0. Further research suggests that these values all represent on-time payments, differing only in payment timing or billing method. Since Kaggle provides no official clarification, it is more reliable to consolidate -2, -1, and 0 into a single category (0) representing on-time payments. This simplification is unlikely to harm predictive performance and may even reduce overfitting by removing arbitrary distinctions between similar cases. Moreover, because these features will be treated as ordinal, collapsing these categories preserves a clear and interpretable ordering where 0 indicates on-time payments and higher values reflect increasing payment delays. It is also conceptually inconsistent to rank -2, -1, and 0 separately, as all indicate timely payment behavior. Therefore, combining them into a single class (0) both simplifies the representation and maintains the intended ordinal structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other features are consistent with the Kaggle documentation. Nevertheless, I will drop the ID and SEX columns: ID provides no useful information for prediction, and including SEX could introduce bias and raise ethical concerns during deployment. Additionally, the large variance observed in the AGE, BILL_AMT, and PAY_AMT features indicates the need for feature scaling to ensure that models relying on distance or magnitude are not unduly influenced by these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of categorical, numerical, and ordinal features. There are no missing entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_df[\"default.payment.next.month\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, with a class distribution of approximately 0.22:0.78 for default and non-default, respectively. This imbalance indicates the need for class weighting or the use of alternative performance metrics during training, since a dummy classifier predicting the majority class could already achieve about 78% accuracy. Therefore, I will use Average Precision as the performance metric, as it captures how effectively the model identifies defaulters in an imbalanced setting. In this context, accurately detecting defaulters is crucial to reducing financial risk, while also minimizing false negatives to ensure fair and equal treatment of clients. Unlike ROC–AUC, which can appear artificially high when the majority class dominates, AP directly evaluates the trade-off between precision and recall. Although metrics such as recall or F1-score could also be appropriate, AP provides a more informative summary of model performance under imbalance by reflecting how well the model balances both types of errors and evaluates performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_status_cols = ['PAY_0', 'PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "bill_amt_cols = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "pay_amt_cols = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "continuous_cols = bill_amt_cols + pay_amt_cols + ['LIMIT_BAL', 'AGE']\n",
    "discrete_cols = pay_status_cols + ['EDUCATION', 'MARRIAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_histogram_continuous(col):\n",
    "    train_df.groupby(\"default.payment.next.month\")[col].plot.hist(bins=30, alpha=0.5, density=True, legend=True, title = \"Histogram of \" + col);\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "    # Here we normalize with density=True so we can compare distribution shapes and proportions \n",
    "    # between groups fairly — independent of how many samples each group contains\n",
    "\n",
    "def plot_discrete_feature(col):\n",
    "    cross_tab = pd.crosstab(train_df[\"default.payment.next.month\"], train_df[col], normalize='index')\n",
    "    cross_tab.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        figsize=(6,4)\n",
    "    )\n",
    "    plt.title(f\"Distribution of {col} within each Default Class\")\n",
    "    plt.xlabel(\"Default (0 = No, 1 = Yes)\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.legend(title=col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    # We create a stacked bar chart to better visualize the proportions relative to each default class\n",
    "    \n",
    "for col in discrete_cols:\n",
    "    plot_discrete_feature(col)\n",
    "\n",
    "for col in continuous_cols:\n",
    "    plot_histogram_continuous(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAY_n:\n",
    "All the histograms for the repayment status features share a similar pattern: the non-default class is dominated by on-time payment categories (-2, -1, 0), with relatively few instances of delayed payments. The default class also shows a majority of on-time payers, but with a noticeably higher proportion of late payments compared to non-defaulters. This is intuitive, as individuals with a history of delayed payments are more likely to default.\n",
    "\n",
    "EDUCATION:\n",
    "The distribution of education levels is similar between defaulters and non-defaulters, though there is a slightly higher proportion of graduate school–educated individuals among non-defaulters. This aligns with expectations, as higher education levels often correlate with higher income and improved financial stability.\n",
    "\n",
    "MARRIAGE:\n",
    "The distribution of marital status is almost identical for both classes, with a slightly higher proportion of married individuals among defaulters. This could be attributed to additional financial responsibilities associated with marriage, such as shared household expenses or dependents.\n",
    "\n",
    "BILL_AMTn:\n",
    "Both defaulters and non-defaulters exhibit highly right-skewed distributions, with most bill amounts concentrated near zero. Defaulters tend to have slightly lower bill amounts on average, suggesting limited predictive power for these features individually but potential usefulness when combined with other behavioral indicators. The non-linear density patterns across the midrange values may pose challenges for linear models such as logistic regression. Toward the high end, non-defaulters dominate—likely reflecting higher creditworthiness and spending confidence—while at the extreme right tail, defaulters are more frequent, possibly representing clients who overextend their credit.\n",
    "\n",
    "PAY_AMTn:\n",
    "Individuals who make larger payments are generally less likely to default, suggesting that higher repayment capacity is a strong indicator of financial reliability.\n",
    "\n",
    "LIMIT_BAL:\n",
    "The distributions of credit limit are right-skewed for both groups. Defaulters have a higher density in the lower credit limit range, whereas non-defaulters are more concentrated at higher limits—consistent with the notion that higher credit limits are extended to more financially stable clients.\n",
    "\n",
    "AGE:\n",
    "Both classes exhibit similarly right-skewed age distributions. Defaulters are more prevalent at the younger and older extremes, while non-defaulters dominate the middle-age range. This may indicate that middle-aged clients, who typically have more stable income and financial experience, are less likely to default. This non-linear relation may again prove difficult for logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.set(font_scale=0.8)\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BILL_AMTn are very highly correlated with each other (correlation coefficients around 0.9–0.95). This means that a client’s bill amount is highly consistent month to month, which is expected as individuals tend to have stable spending patterns and credit limits. However, these high correlations also imply multicollinearity, which can cause instability in models like logistic regression. To reduce this impact we can take aggregates during feature engineering.\n",
    "\n",
    "The PAY_n columns also exhibit moderate correlations (0.5–0.8). This indicates that people who are late in one month are likely to be late again. PAY_0 (September) is the most correlated with the target variable, suggesting that recent repayment behavior is the strongest predictor of default.\n",
    "\n",
    "Each PAY_AMTn has moderate positive correlation (about 0.2–0.3) with the corresponding BILL_AMTn. This reflects that people generally pay in proportion to how much they owe as found before. Nonetheless, BILL_AMTn and PAY_AMTn show very small or negative correlations, suggesting that raw monetary values don’t directly indicate default without context (e.g. relative to limits or delays) pointing to the need for feature engineering and non-linear models.\n",
    "\n",
    "The demographic features EDUCATION, MARRIAGE and AGE show weak correlation (<0.2) with the target. While these may still add minor predictive value, it indicates that default behavior is driven more by payment patterns than demographics.\n",
    "\n",
    "LIMIT_BAL exhibits a small negative correlation (approximately -0.15) with default — higher credit limits are slightly associated with lower risk as proposed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4. Feature engineering\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform feature engineering**: Create new features that are relevant to the problem and use this updated feature set in the following exercises. You may need to iterate between **feature engineering** and **preprocessing** to refine your features and improve model performance.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the data cleaning and engineering discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"EDUCATION\"] = train_df[\"EDUCATION\"].replace({6: 5})\n",
    "train_df = train_df.drop(train_df[train_df['MARRIAGE']==0].index)\n",
    "train_df = train_df.drop(train_df[train_df['EDUCATION']==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"EDUCATION\"] = test_df[\"EDUCATION\"].replace({6: 5})\n",
    "test_df = test_df.drop(test_df[test_df['MARRIAGE']==0].index)\n",
    "test_df = test_df.drop(test_df[test_df['EDUCATION']==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pay_status_cols:\n",
    "    train_df.loc[train_df[p]<=0, p] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered discretizing the BILL_AMT and PAY_AMT features. However, this approach could lead to overfitting, since there is no strong reason to assume that individuals in adjacent bins differ meaningfully in credit behavior. At the same time, keeping these features continuous may also be problematic for logistic regression, as a single weight could unrealistically assign higher risk to clients with larger bill or payment amounts. Thus, we will leave these features continuous and apply scaling to limit the effect of magnitude differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['TOTAL_PAY_AMT'] = train_df[pay_amt_cols].sum(axis=1)\n",
    "train_df['TOTAL_BILL_AMT'] = train_df[bill_amt_cols].sum(axis=1)\n",
    "train_df['TOTAL_PAY_RATIO'] = np.where(train_df['TOTAL_BILL_AMT'] == 0,\n",
    "    1, train_df['TOTAL_PAY_AMT'] / train_df['TOTAL_BILL_AMT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create TOTAL_PAY_AMT, TOTAL_BILL_AMT, and their ratio TOTAL_PAY_RATIO = TOTAL_PAY_AMT / TOTAL_BILL_AMT to capture a client’s overall repayment coverage across six months. Using a ratio makes logistic regression less sensitive to absolute magnitudes (large bills/payments) and focuses the model on repayment behaviour rather than raw payment amounts. Aggregating over all months smooths timing mismatches (e.g. a bill in month t paid in t+1), reduces monthly noise, and reflects long-term behavior instead of one-off spikes. When TOTAL_BILL_AMT = 0, we set the ratio to 1 to avoid infinities and to encode the “fully covered” situation; alternatively, this could be set to NaN and imputed, but treating it as 1 preserves the repayment-adequacy interpretation. Overall, TOTAL_PAY_RATIO provides a scale-robust, interpretable signal that helps prevent the model from favoring clients merely because they make large absolute payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['AVG_DELAY'] = train_df[pay_status_cols].mean(axis=1)\n",
    "train_df['MAX_DELAY'] = train_df[pay_status_cols].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we aggregate delay behaviour to serve as a strong predictor for linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 7):\n",
    "    train_df[f'BILL_GROWTH{i}'] = train_df[f'BILL_AMT{i}'] - train_df[f'BILL_AMT{i-1}']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bill growth  measures the monthly change in billed amounts and serves as a useful indicator of piling debt or changing spending behavior. However, individuals with higher credit limits naturally experience greater variance in monthly bills, introducing a non-linear relationship between bill growth and risk of default. Linear models may struggle to capture this effect, but feature scaling can help mitigate magnitude differences across clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Preprocessing and transformations\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Identify feature types**: Determine the different types of features in your dataset (e.g., numerical, categorical, ordinal, text) and specify the transformations you would apply to each type.\n",
    "\n",
    "2. **Define a column transformer (if needed)**: Implement a `ColumnTransformer` to apply the appropriate preprocessing steps to each feature type.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['default.payment.next.month'])\n",
    "y_train = train_df['default.payment.next.month']\n",
    "\n",
    "X_test = test_df.drop(columns=['default.payment.next.month'])\n",
    "y_test = test_df['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = (\n",
    "    bill_amt_cols\n",
    "    + pay_amt_cols\n",
    "    + [\n",
    "        'LIMIT_BAL',\n",
    "        'AGE',\n",
    "        'TOTAL_PAY_AMT',\n",
    "        'TOTAL_BILL_AMT',\n",
    "        'TOTAL_PAY_RATIO',\n",
    "        'AVG_DELAY',\n",
    "        'MAX_DELAY',\n",
    "        'BILL_GROWTH2',\n",
    "        'BILL_GROWTH3',\n",
    "        'BILL_GROWTH4',\n",
    "        'BILL_GROWTH5',\n",
    "        'BILL_GROWTH6',\n",
    "    ]\n",
    ")\n",
    "categorical_features = ['MARRIAGE']\n",
    "ordinal_features = pay_status_cols + ['EDUCATION']\n",
    "drop_features = ['ID', 'SEX']\n",
    "\n",
    "education_order = [1, 2, 3, 4, 5]    \n",
    "pay_status_order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "\n",
    "ordinal_transformer = OrdinalEncoder(\n",
    "    categories=[pay_status_order] * len(pay_status_cols) + [education_order],\n",
    "    dtype=int\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "ordinal_transformer = OrdinalEncoder(\n",
    "    categories=[pay_status_order] * len(pay_status_cols) + [education_order],\n",
    "    dtype=int\n",
    ")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer, ordinal_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Baseline model\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Establish a baseline**: Use one of `scikit-learn`’s baseline models (e.g., `DummyClassifier` or `DummyRegressor`, depending on your task) and report the results. This will serve as a reference point for evaluating the performance of your more advanced models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"average_precision\"]\n",
    "results_dict = {}\n",
    "\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    return mean_scores, std_scores\n",
    "\n",
    "def print_results(mean_scores, std_scores):\n",
    "    out_col = [] \n",
    "    for i in range(len(mean_scores)): \n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i]))) \n",
    "    return pd.Series(data=out_col, index=mean_scores.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(random_state = 123)\n",
    "pipe_dummy = make_pipeline(preprocessor, dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "mean_scores, std_scores = mean_std_cross_val_scores(pipe_dummy, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "results_dict['dummy'] = print_results(mean_scores, std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dummy achieves an accuracy equal to the proportion of the non-default class but low AP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 7. Linear models \n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Train a linear model**: Use a linear model as your first real attempt at solving the problem.\n",
    "\n",
    "2. **Tune hyperparameters**: Perform hyperparameter tuning to explore different values of the model's complexity parameter. \n",
    "\n",
    "3. **Evaluate with cross-validation**: Report the cross-validation scores along with their standard deviation.\n",
    "\n",
    "4. **Summarize findings**: Summarize your results, highlighting key observations from your experiments.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try a default logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(max_iter=1000, random_state=123),\n",
    ")\n",
    "mean_scores, mean_std = mean_std_cross_val_scores(pipe_lr, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "results_dict[\"lr_default\"] = print_results(mean_scores, mean_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are better than the baseline but we hope to improve it with some optimization of the hyperparameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "params_lr = {'C': [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "train_acc_scores = []\n",
    "cv_acc_scores = []\n",
    "train_ap_scores = []\n",
    "cv_ap_scores = []\n",
    "best_ap = 0\n",
    "best_C = None\n",
    "\n",
    "for c in params_lr['C']:\n",
    "    pipe_lr = make_pipeline(preprocessor, LogisticRegression(C=c, max_iter=1000, random_state=123))\n",
    "    mean_scores, mean_std = mean_std_cross_val_scores(pipe_lr, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "    train_acc_scores.append(mean_scores[\"train_accuracy\"])\n",
    "    cv_acc_scores.append(mean_scores[\"test_accuracy\"])\n",
    "    train_ap_scores.append(mean_scores[\"train_average_precision\"])\n",
    "    cv_ap_scores.append(mean_scores[\"test_average_precision\"])\n",
    "    results_dict[f\"lr_C_{c}\"] = print_results(mean_scores, mean_std)\n",
    "    if (mean_scores[\"test_average_precision\"] > best_ap):\n",
    "        best_C = c\n",
    "        best_ap = mean_scores[\"test_average_precision\"]\n",
    "\n",
    "plt.semilogx((params_lr['C']), train_acc_scores, label=\"train accuracy\")\n",
    "plt.semilogx((params_lr['C']), cv_acc_scores, label=\"valid accuracy\")\n",
    "plt.semilogx((params_lr['C']), train_ap_scores, label=\"train average precision\")\n",
    "plt.semilogx((params_lr['C']), cv_ap_scores, label=\"valid average precision\")\n",
    "plt.legend()\n",
    "plt.title(\"Logistic Regression Performance vs C\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"performance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not seeing much improvement when changing the value of C. This suggests that further reducing regularization does not improve performance and the model has reached its capacity to fit the data; the data is likely not linearly seperable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now further split the training set to create a validation set that will be used for generating confusion matrices. This is not the best practice as the validation set was used during the determination of the best hyperparameter value. Nevertheless, we use the validation set purely to create confusion matrices and compare them between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small, X_valid, y_train_small, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cm(pipe):\n",
    "    pipe.fit(X_train_small, y_train_small)\n",
    "    cm = ConfusionMatrixDisplay.from_estimator(pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"])\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_best = make_pipeline(preprocessor, LogisticRegression(C=best_C, max_iter=1000, random_state=123))\n",
    "make_cm(pipe_lr_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of false negatives. This is expected since the dataset is imbalanced. To counteract this, we can set the weights to balanced which will most likely decrease precision but increase recall. We will experiment with this hyperparameter later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 8. Different models\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Experiment with additional models**: Train at least **three models** other than a linear model. Ensure that **at least one** of these models is a **tree-based ensemble model** (e.g., Random Forest, Gradient Boosting, or XGBoost).\n",
    "\n",
    "2. **Compare and interpret results**: Summarize your findings in terms of **overfitting/underfitting** behavior and **fit/score times** for each model. Reflect on your results. Were you able to **outperform the linear model**?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_8\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with KBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_svm = make_pipeline(preprocessor, SVC(random_state=123))\n",
    "mean_scores, mean_std = mean_std_cross_val_scores(pipe_svm, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "results_dict[\"svm_default\"] = print_results(mean_scores, mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cm(pipe_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_knn = make_pipeline(preprocessor, KNeighborsClassifier())\n",
    "mean_scores, mean_std = mean_std_cross_val_scores(pipe_knn, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "results_dict[\"knn_default\"] = print_results(mean_scores, mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cm(pipe_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\n",
    "mean_scores, mean_std = mean_std_cross_val_scores(pipe_rf, X_train, y_train, cv=5, return_train_score=True, scoring=scoring)\n",
    "results_dict[\"rf_default\"] = print_results(mean_scores, mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cm(pipe_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 9. Feature selection \n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform feature selection**: Attempt to select relevant features using methods such as `RFECV` or forward selection.\n",
    "\n",
    "2. **Evaluate the impact** Compare the model performance before and after feature selection. Do the results improve with feature selection?\n",
    "\n",
    "3. **Summarize findings** Summarize your observations and decide whether to **keep feature selection** in your pipeline.  If it improves results, retain it for the next exercises; otherwise, you may choose to omit it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_9\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "feature_selector = RFECV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5,\n",
    "    scoring=\"average_precision\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "rfe_pipeline = make_pipeline(preprocessor, feature_selector)\n",
    "rfe_pipeline.fit(X_train, y_train)\n",
    "rfe_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "feature_mask = rfe_pipeline.named_steps['rfecv'].support_\n",
    "feature_names = rfe_pipeline.named_steps['columntransformer'].get_feature_names_out()\n",
    "print(\"All features: \")\n",
    "print(feature_names.tolist())\n",
    "selected_features = feature_names[feature_mask]\n",
    "print(\"Selected features: \")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 10. Hyperparameter optimization\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Optimize hyperparameters**: Attempt to optimize hyperparameters for the models you have tried so far. In at least **one case**, tune **multiple hyperparameters** for a single model.\n",
    "\n",
    "2. **Use suitable optimization methods**: You may use any of the following approaches for hyperparameter optimization:\n",
    "   - [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)  \n",
    "   - [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n",
    "   - [Bayesian optimization with scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)\n",
    "\n",
    "3. **Summarize your results**: Report and compare the optimized results across models. Discuss whether hyperparameter optimization led to performance improvements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_10\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "For logistic regression, we will jointly optimize the C and class_weight that control the fundamental trade-off and class importance,respectively. We could also manually optimize the threshold that controls the  minimum probability at which class 1 (default) is predicted, but we will instead look at the resultant PR curve for the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, pipe):\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        n_jobs=-1,\n",
    "        scoring=scoring,\n",
    "        refit='average precision',\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"logisticregression__C\": loguniform(1e-2, 15),\n",
    "    \"logisticregression__class_weight\": [\n",
    "        None,\n",
    "        \"balanced\",\n",
    "        {0:1, 1:1},    \n",
    "        {0:1, 1:2},    \n",
    "        {0:1, 1:3},\n",
    "        {0:1, 1:5},\n",
    "        {0:1, 1:10}\n",
    "    ],\n",
    "}\n",
    "random_search_lr = random_search(pipe_lr_best, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(search.cv_results_).set_index(\"rank_test_score\").sort_index().T\n",
    "pd.DataFrame(random_search_lr.cv_results_).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot CM and PR curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM RBF we will jointly optimize gamma and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__C\": loguniform(1e-2, 1e2),\n",
    "    \"svc__gamma\": [ \"scale\", loguniform(1e-4, 1e0) ],\n",
    "}\n",
    "\n",
    "random_search_svm = random_search(pipe_svm, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search_svm.cv_results_).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kNN we will jointly optimize n_neighbours and the number of features used (dimensionality reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpipe_knn\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'pipe_knn' is not defined"
     ]
    }
   ],
   "source": [
    "pipe_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"selectfrommodel__max_features\": randint(10, 300),\n",
    "    \"selectfrommodel__estimator__max_depth\": [None, 8, 12, 16],\n",
    "    \"selectfrommodel__estimator__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"kneighborsclassifier__n_neighbors\": randint(3, 75),\n",
    "}\n",
    "\n",
    "pipe_knn_sfm = make_pipeline(\n",
    "    preprocessor,                             \n",
    "    SelectFromModel(RandomForestClassifier(\n",
    "        n_estimators=300, random_state=123, n_jobs=-1\n",
    "    )),\n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "\n",
    "random_search_knn = random_search(pipe_knn_sfm, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search_knn.cv_results_).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random forest classifier we will jointly optimize the n_estimators, max_features, and max_depth hyperparameters that control the bias/variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"randomforestclassifier__n_estimators\": randint(100, 500),   # number of trees\n",
    "    \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"randomforestclassifier__max_depth\": [None, 5, 10, 20, 30, 50],\n",
    "}\n",
    "\n",
    "random_search_rf = random_search(pipe_rf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search_rf.cv_results_).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 11. Interpretation and feature importances\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Interpret model feature importance**: Use one of the interpretation methods discussed in class (e.g., `shap`), or another suitable method of your choice, to examine the most important features of one of your **non-linear models**.\n",
    "\n",
    "2. **Summarize insights**: Summarize your observations about which features contribute most to the model's predictions and how they influence the outcomes.\n",
    "\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_11\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "We choose to examine the feature importances of our best-performing random forest classifier with sklearn's feature_importances_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare these with the coefficients learned by our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 12. Results on the test set\n",
    "<hr>\n",
    "\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Evaluate on the test set**: Apply your best-performing model to the test data and report the test scores.\n",
    "\n",
    "2. **Compare and reflect**: Compare the **test scores** with the **validation scores** from previous experiments. Discuss the consistency between them. How much do you **trust your results**? Reflect on whether you might have encountered **optimization bias**.\n",
    "\n",
    "3. **Explain individual predictions**: Select one or two examples from your test predictions and use an interpretation method (e.g., **SHAP force plots**) to explain these individual predictions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_12\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 13. Summary of results\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "Imagine you are preparing to present the summary of your results to your boss and co-workers.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Summarize key results**: Create a clear and concise table highlighting your most important results (e.g., models compared, validation/test scores, key observations).\n",
    "\n",
    "2. **Write concluding remarks**: Summarize your main takeaways from the project, including what worked well and what did not.\n",
    "\n",
    "3. **Propose future improvements**: Discuss ideas or approaches you did not try but that could potentially improve **performance** or **interpretability**.\n",
    "\n",
    "4. **Report final results**: Report your **final test score** and the **metric** you used.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_13\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 14. Your takeaway\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from the supervised machine learning material we have learned so far? Please write thoughtful answers.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_14\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ve learned that training supervised machine learning models is less about building complex algorithms and more about understanding and shaping the data that feeds them. The most important work often happens before model training — in data cleaning, feature engineering, and thoughtful preprocessing based on each feature’s characteristics. I also realized how much performance depends on experimenting with different models and tuning hyperparameters to find the best fit for the problem. Beyond accuracy, it’s equally crucial to interpret model behavior and understand which features drive its decisions, especially in high-stakes contexts where black-box models are not acceptable. Overall, my biggest takeaway is that successful machine learning relies as much on critical thinking and data understanding as it does on technical modeling skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please ensure you have followed all the steps in the **Instructions** section at the top.  \n",
    "\n",
    "### Submission checklist  \n",
    "\n",
    "- [ ] Restart the kernel and run all cells (▶▶ button)\n",
    "- [ ] Make sure to push the most up to date version of your homework assignment to your GitHub repository so that we can use it for grading if there are any problems with your submission on Gradescope. \n",
    "- [ ] The `.ipynb` file runs without errors and shows all outputs.  \n",
    "- [ ] Only the `.ipynb` file and required output files are uploaded (no extra files).  \n",
    "- [ ] If the `.ipynb` file is too large to render on Gradescope, upload a Web PDF and/or HTML version as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky one but you did it 👏👏!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
